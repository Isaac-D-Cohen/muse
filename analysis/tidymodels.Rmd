---
title: "Getting started with tidymodels"
date: "`r Sys.Date()`"
output:
  workflowr::wflow_html:
    toc: true
editor_options:
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Build a model

https://www.tidymodels.org/start/models/

```{r load_packages, message=FALSE, warning=FALSE}
my_packages <- c('tidyverse', 'tidymodels', 'broom.mixed', 'dotwhisker', 'rstanarm', 'mlbench', 'ranger', 'randomForest', 'rpart.plot', 'vip', 'modeldata')

for (my_package in my_packages){
   if(!require(my_package, character.only = TRUE)){
      install.packages(my_package)
   }
  library(my_package, character.only = TRUE)
}

theme_set(theme_bw())
```

Data from Constable (1993) to explore how three different feeding regimes affect the size of sea urchins over time. The initial size of the sea urchins at the beginning of the experiment probably affects how big they grow as they are fed.

```{r read_csv}
my_csv <- read_csv("https://tidymodels.org/start/models/urchins.csv", show_col_types = FALSE)
head(my_csv)
```

Change the column names to be more descriptive and change `food_regime` into a factor.

For each of the 72 urchins, we know their:

* experimental feeding regime group (`food_regime`: either `Initial`, `Low`, or `High`),
* size in milliliters at the start of the experiment (`initial_volume`), and
* suture width at the end of the experiment (`width`).

```{r urchins}
my_csv %>%
  setNames(c("food_regime", "initial_volume", "width")) %>% 
  mutate(food_regime = factor(food_regime, levels = c("Initial", "Low", "High"))) -> urchins
dim(urchins)
```

Plot the data.

```{r plot_urchins}
ggplot(
  urchins,
  aes(x = initial_volume, 
      y = width, 
      group = food_regime, 
      colour = food_regime)
) + 
  geom_point() + 
  geom_smooth(method = lm, se = FALSE) +
  scale_color_viridis_d(option = "plasma", end = .7)
```

A standard two-way analysis of variance (ANOVA) model makes sense for this dataset because we have both a continuous predictor (`initial_volume`) and a categorical predictor (`food_regime`). Since the slopes appear to be different for at least two of the feeding regimes, let's build a model that allows for two-way interactions.

Specifying an R formula with our variables as defined below allows our regression model depending on initial volume to have separate slopes and intercepts for each food regime.

```{r formula}
width ~ initial_volume * food_regime
```

With `tidymodels`, we start by specifying the functional form of the model that we want by using the `parsnip` package. Since there is a numeric outcome and the model should be linear with slopes and intercepts, the model type is "linear regression".

```{r linear_reg}
linear_reg()
```

Now we can think about a method for fitting or training the model, i.e. the model engine. The engine value is often a mash-up of the software that can be used to fit or train the model as well as the estimation method. The default for `linear_reg()` is `lm` for ordinary least squares. The documentation page for `linear_reg()` lists all the possible engines.

We'll save our model object as `lm_mod` using the default engine.

```{r lm_mod}
lm_mod <- linear_reg()
```

The model can now be estimated or trained using the `fit()` function.

```{r lm_fit}
lm_mod %>%
  fit(width ~ initial_volume * food_regime, data = urchins) -> lm_fit

lm_fit
```

Many models have a `tidy()` method that provides the summary results in a more predictable and useful format.

```{r tidy_lm}
tidy(lm_fit)
```

Generate a dot-and-whisker plot of our regression results using the `dotwhisker` package.

```{r dwplot}
tidy(lm_fit) %>% 
  dwplot(
    dot_args = list(size = 2, color = "black"),
    whisker_args = list(color = "black"),
    vline = geom_vline(
      xintercept = 0,
      colour = "grey50",
      linetype = 2
    )
  )
```

The fitted object `lm_fit` has the `lm` model output built-in, which you can access with `lm_fit$fit`, but there are some benefits to using the fitted `parsnip` model object when it comes to predicting.

Suppose that, for a publication, it would be particularly interesting to make a plot of the mean body size for urchins that started the experiment with an initial volume of 20ml. To create such a graph, we start with some new example data that we will make predictions for.

```{r new_points}
new_points <- expand.grid(
  initial_volume = 20, 
  food_regime = c("Initial", "Low", "High")
)

new_points
```

Generate the mean body width values.

```{r}
mean_pred <- predict(lm_fit, new_data = new_points)
mean_pred
```

When making predictions, the tidymodels convention is to always produce a tibble of results with standardized column names. This makes it easy to combine the original data and the predictions in a usable format.

```{r conf_int_pred}
conf_int_pred <- predict(
  lm_fit, 
  new_data = new_points, 
  type = "conf_int"
)

plot_data <- 
  new_points %>% 
  bind_cols(mean_pred) %>% 
  bind_cols(conf_int_pred)

ggplot(
  plot_data,
  aes(x = food_regime)
) + 
  geom_point(aes(y = .pred)) + 
  geom_errorbar(
    aes(ymin = .pred_lower, ymax = .pred_upper),
    width = .2
  ) + 
  labs(y = "urchin size")
```

We are interested in knowing if the results would be different if the model were estimated using a Bayesian approach. In such an analysis, a prior distribution needs to be declared for each model parameter that represents the possible values of the parameters (before being exposed to the observed data). After some discussion, the group agrees that the priors should be bell-shaped but, since no one has any idea what the range of values should be, to take a conservative approach and make the priors wide using a Cauchy distribution (which is the same as a t-distribution with a single degree of freedom).

The documentation on the `rstanarm` package shows us that the `stan_glm()` function can be used to estimate this model, and that the function arguments that need to be specified are called `prior` and `prior_intercept`.

It turns out that `linear_reg()` has a `stan` engine. Since these prior distribution arguments are specific to the Stan software, they are passed as arguments to `parsnip::set_engine()`.

```{r bayes_mod}
# set the prior distribution
prior_dist <- rstanarm::student_t(df = 1)

set.seed(123)

# make the parsnip model
bayes_mod <-   
  linear_reg() %>% 
  set_engine("stan", 
             prior_intercept = prior_dist, 
             prior = prior_dist) 

# train the model
bayes_fit <- 
  bayes_mod %>% 
  fit(width ~ initial_volume * food_regime, data = urchins)

print(bayes_fit, digits = 5)
```

To update the parameter table, the `tidy()` method is once again used.

```{r tidy_bayes_fit}
tidy(bayes_fit, conf.int = TRUE)
```

A goal of the `tidymodels` packages is that the interfaces to common tasks are standardised (as seen in the `tidy()` results above). The same is true for getting predictions; we can use the same code even though the underlying packages use very different syntax.

```{r}
bayes_plot_data <- 
  new_points %>% 
  bind_cols(predict(bayes_fit, new_data = new_points)) %>% 
  bind_cols(predict(bayes_fit, new_data = new_points, type = "conf_int"))

ggplot(bayes_plot_data, aes(x = food_regime)) + 
  geom_point(aes(y = .pred)) + 
  geom_errorbar(aes(ymin = .pred_lower, ymax = .pred_upper), width = .2) + 
  labs(y = "urchin size") + 
  ggtitle("Bayesian model with t(1) prior distribution")
```

## Sonar data

Load and [split data](https://www.tidymodels.org/start/resampling/#data-split) using [rsample](https://rsample.tidymodels.org/).

The `initial_split()` function takes the original data and saves the information on how to make the partitions. 

The `strata` argument conducts a stratified split ensuring that our training and test data sets will keep roughly the same proportion of classes.

```{r sonar}
data(Sonar, package = "mlbench")

set.seed(1984)
sonar_split <- initial_split(data = Sonar, prop = 0.8, strata = 'Class')
sonar_train <- training(sonar_split)
sonar_test <- testing(sonar_split)
```

## `parsnip`

The [parsnip](https://parsnip.tidymodels.org/index.html) package provides a tidy and unified interface to a range of models.

```{r train_rf}
my_mtry <- ceiling(sqrt(ncol(Sonar)))

rf_ranger <- list()

rand_forest(mtry = my_mtry, trees = 500) %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification") -> rf_ranger$model

rf_ranger$model %>%
  fit(Class ~ ., data = sonar_train) -> rf_ranger$fit

rf_randomForest <- list()

rand_forest(mtry = my_mtry, trees = 500) %>%
  set_engine("randomForest") %>%
  set_mode("classification") -> rf_randomForest$model

rf_randomForest$model %>%
  fit(Class ~ ., data = sonar_train) -> rf_randomForest$fit
```

## `yardstick`

Example data to check how to prepare our data.

```{r two_class_example}
data(two_class_example)
str(two_class_example)
```

Predict and generate table in the format of `two_class_example`.

```{r predict}
predict_wrapper <- function(fit, test_data, type = 'prob'){
  predict(fit, test_data, type = type) %>%
    mutate(truth = test_data$Class) %>%
    rename(
      M = .pred_M,
      R = .pred_R
    ) %>%
    mutate(predicted = as.factor(ifelse(M > 0.5, 'M', 'R'))) %>%
    select(truth, everything())
}

rf_ranger$predictions <- predict_wrapper(rf_ranger$fit, sonar_test)
rf_randomForest$predictions <- predict_wrapper(rf_randomForest$fit, sonar_test)
```

Area under the PR curve.

```{r pr_auc}
pr_auc(rf_ranger$predictions, truth, M)
pr_auc(rf_randomForest$predictions, truth, M)
```

[PR curve](https://yardstick.tidymodels.org/reference/pr_curve.html).

```{r pr_curve}
pr_curve(rf_ranger$predictions, truth, M) %>%
  ggplot(aes(x = recall, y = precision)) +
  geom_path() +
  coord_equal() +
  ggtitle('PR curve (ranger package model)')

pr_curve(rf_randomForest$predictions, truth, M) %>%
  ggplot(aes(x = recall, y = precision)) +
  geom_path() +
  coord_equal() +
  ggtitle('PR curve (randomForest package model)')
```

Area under the ROC curve.

```{r roc_auc}
roc_auc(rf_ranger$predictions, truth, M)
roc_auc(rf_randomForest$predictions, truth, M)
```

[ROC curve](https://yardstick.tidymodels.org/reference/roc_curve.html).

```{r roc_curve}
roc_curve(rf_ranger$predictions, truth, M) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  ggtitle('ROC curve (ranger package model)')

roc_curve(rf_randomForest$predictions, truth, M) %>%
  ggplot(aes(x = 1 - specificity, y = sensitivity)) +
  geom_path() +
  geom_abline(lty = 3) +
  coord_equal() +
  ggtitle('ROC curve (randomForest package model)')
```

## Resampling

Resampling methods, such as cross-validation and the bootstrap, are empirical simulation systems. They create a series of data sets similar to the training/testing split discussed previously; a subset of the data are used for creating the model and a different subset is used to measure performance. **Resampling is always used with the training set**.

Cross-validation folds can be created using `vfold_cv()`. Here we perform a 5-fold cross-validation, which splits the data into 80/20, repeated 3 times.

```{r vfold_cv}
set.seed(1984)
folds <- vfold_cv(sonar_train, v = 5, repeats = 3)
folds
```

Use `workflow()` to bundle a model and formula.

```{r workflow}
workflow() %>%
  add_model(rf_ranger$model) %>%
  add_formula(Class ~ .) -> rf_ranger$wf

set.seed(1984)
rf_ranger$wf %>%
  fit_resamples(folds) -> rf_ranger$fit_rs

workflow() %>%
  add_model(rf_randomForest$model) %>%
  add_formula(Class ~ .) -> rf_randomForest$wf

set.seed(1984)
rf_randomForest$wf %>%
  fit_resamples(folds) -> rf_randomForest$fit_rs
```

Metrics.

```{r collect_metrics}
collect_metrics(rf_ranger$fit_rs)
collect_metrics(rf_randomForest$fit_rs)
```

Compare with single sample.

```{r roc_auc_again}
roc_auc(rf_ranger$predictions, truth, M)
roc_auc(rf_randomForest$predictions, truth, M)
```

## Tuning

Some model parameters cannot be learned directly from a data set during model training; these kinds of parameters are called hyperparameters. Instead of learning these kinds of hyperparameters during model training, we can estimate the best values for these values by training many models on resampled data sets and exploring how well all these models perform. This process is called tuning.

Prepare data.

```{r prepare_cells_data}
data(cells, package = "modeldata")

set.seed(123)
cell_split <- initial_split(
  cells %>% select(-case), 
  strata = class
)
cell_train <- training(cell_split)
cell_test  <- testing(cell_split)
```

Tune `cost_complexity` and `tree_depth`. Use `args()` to see which parsnip object arguments are available for tuning.

```{r tune_spec}
tune_spec <- 
  decision_tree(
    cost_complexity = tune(),
    tree_depth = tune()
  ) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

tune_spec
```

Create grid using `grid_regular`, which chooses sensible values to try for each hyperparameter. Since we have two parameters to tune, there are 25 different combinations.

```{r tree_grid}
tree_grid <- grid_regular(
  cost_complexity(),
  tree_depth(),
  levels = 5
)

tree_grid
```

Create cross-validation folds (default is 10-fold).

```{r cell_folds}
set.seed(234)
cell_folds <- vfold_cv(cell_train)
cell_folds
```

Workflow.

```{r tree_wf}
set.seed(345)

tree_wf <- workflow() %>%
  add_model(tune_spec) %>%
  add_formula(class ~ .)

system.time(
  tree_res <- 
    tree_wf %>% 
    tune_grid(
      resamples = cell_folds,
      grid = tree_grid
    )
)
```

Show best and select best metrics.

```{r tree_res_show_best}
tree_res %>%
  show_best("roc_auc")

tree_res %>%
  select_best("roc_auc")
```

Plot.

```{r tree_res_plot}
tree_res %>%
  collect_metrics() %>%
  mutate(tree_depth = factor(tree_depth)) %>%
  ggplot(aes(cost_complexity, mean, color = tree_depth)) +
  geom_line(linewidth = 1.5, alpha = 0.6) +
  geom_point(size = 2) +
  facet_wrap(~ .metric, scales = "free", nrow = 2) +
  scale_x_log10(labels = scales::label_number()) +
  scale_color_viridis_d(option = "plasma", begin = .9, end = 0)
```

Finalise.

```{r final_wf}
tree_res %>%
  select_best("accuracy") -> best_tree

final_wf <- 
  tree_wf %>% 
  finalize_workflow(best_tree)

final_wf
```

Last fit using `last_fit`, which _fits_ the finalised model on the full training data set and _evaluates_ the finalised model on the testing data.

```{r last_fit}
final_fit <- 
  final_wf %>%
  last_fit(cell_split) 

final_fit %>%
  collect_metrics()

final_fit %>%
  collect_predictions() %>% 
  roc_curve(class, .pred_PS) %>% 
  autoplot()
```

The `final_fit` object contains a finalised, fitted workflow that you can use for predicting on new data or further understanding the results. You may want to extract this object, using one of the `extract_` helper functions.

```{r final_tree}
final_tree <- extract_workflow(final_fit)
final_tree %>%
  extract_fit_engine() %>%
  rpart.plot(roundint = FALSE)
```

Variable importance.

```{r vip}
final_tree %>% 
  extract_fit_parsnip() %>% 
  vip()
```
